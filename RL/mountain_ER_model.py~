import random
import numpy as np
from keras import backend as K
from keras.optimizers import Adam
import tensorflow as tf
from collections import deque
import copy
import matplotlib.pyplot as plt

"""
experience Replay　　　　　◎
Fixed Target Q-Network　　◎
Reward Clipping　　　　　　　
Huber Loss　　　　　　　　　◎
"""
# huberloss (loss関数)
def huberloss(y_true, y_pred):
    err = y_true - y_pred
    cond = K.abs(err) < 1.0
    L2 = 0.5 * K.square(err)
    L1 = (K.abs(err) - 0.5)
    loss = tf.where(cond, L2, L1)  # Keras does not cover where function in tensorflow :-(
    return K.mean(loss)

# MountainCar-v0環境の構築
import gym
env = gym.make('MountainCar-v0')

# 深層強化学習環境の構築

from keras.models import Sequential
from keras.layers import Dense, Activation

class Qnet:
    def __init__(self):
        self.model = Sequential()
        self.model.add(Dense(24, input_dim = 2, activation='relu'))
        self.model.add(Dense(24, activation='relu'))
        self.model.add(Dense(24, activation='relu'))
        self.model.add(Dense(3, activation='linear'))
        self.model.compile(optimizer=Adam(lr=0.001), loss=huberloss,metrics=['categorical_accuracy'])

    def copy_weights(self,model):
        model.save_weights('model_copy_weights.h5')
        self.model.load_weights('model_copy_weights.h5')
        
EPOCH = 10000
ITER = 200
EPSIL = 0.2
gamma = 0.99
alpha = 0.2
initialize_size = 2000 #この大きさを超えてから学習開始
max_size = 20000 #キューの大きさ
convert_size = 3 # ターゲットQの変更頻度
batch_size = 32


main = Qnet()
target = Qnet()

memory = deque(maxlen=max_size)
reward_list = []

def get_next_action_max_arg(model,observation):
    x = np.array([observation])
    reward = model.predict(x)
    a_max = np.argmax(reward)
    return a_max

def get_next_action_max_value(model,observation):
    x = np.array([observation])
    reward = model.predict(x)
    return np.amax(reward)


for i in range(EPOCH):

    #エージェントの初期化
    observation = env.reset()

    done = False
    rew = 0

    target.copy_weights(main.model)

    pos_lis = []

    iter = 1

    #進行度を表示
    """
    if i % 10 == 0:
        print(str(i)+"EPISODE finished")
    """
    # 0:左 1:そのまま　2:右
    action = env.action_space.n

    while not done:
        if i % 100 == 0:
            env.render()
        observation_now = observation

        #####　行動決定　####
        rand = random.random()
        eps = EPSIL - 0.000025 * i
        if eps < 0.1:
            eps = 0.1
        if rand < eps:
            next_action = env.action_space.sample()
        else:
            next_action = get_next_action_max_arg(main.model,observation)

        #####################

        ####  状態遷移、報酬取得  ####

        observation, reward, done, info = env.step(next_action)

        #### Reward Clipping ####

        ############################

        ####  報酬をメモリへ　####
        memory.append((observation_now,next_action,reward,observation))
        ####  パラメータ更新  ####

        if len(memory) >= initialize_size:
            inputs = np.zeros((batch_size,2))
            targets = np.zeros((batch_size,3))
            #replace 重複
            rand = np.random.choice(np.arange(len(memory)),size=batch_size,replace=False)
            batch_train = [memory[i] for i in rand]
            for j, (state_b, action_b, reward_b, next_state_b) in enumerate(batch_train):
                inputs[j] = state_b
                targets[j] = main.model.predict(np.array([state_b]))
                n_a = get_next_action_max_arg(target.model,next_state_b)
                if next_state_b[0] >= 0.5:
                    q = reward_b
                else:
                    q = reward_b + gamma * target.model.predict(np.array([next_state_b]))[0][n_a]
                targets[j][action_b] = q
            #print(inputs)
            #print(targets)
            main.model.fit(inputs,targets,epochs=1,verbose=0)


        if iter % convert_size == 0:
            target.copy_weights(main.model)
        ######################

        """
        if observation[0] >= 0.:
            print("x posion")
            print(observation[0])
            print(str(i)+"エピソード")
            print("登頂成功！！")
        """

        iter += 1
        rew += reward
        pos_lis.append(observation[0])


    reward_list.append(rew)
    print(str(i)+" : EPISODE finished")
    print(max(pos_lis))

    if i % 250 == 0:
        str_i = str(i)
        main.model.save_weights('param/model_'+str_i+'_epi_24_24_24_fix.h5')
# env = wrappers.Monitor(env, './movie/cartpoleDDQN')  # 動画保存する場合


print(reward_list)

x_len = len(reward_list)
x = np.arange(0, x_len, 1)
x = np.array(x)
y = np.array(reward_list)

plt.plot(x,y)
plt.savefig('MountainCar.png')
